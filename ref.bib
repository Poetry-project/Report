%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Ziyad Alsaeed at 2023-03-04 15:56:04 +0300 


%% Saved with string encoding Unicode (UTF-8) 



@article{Chang:Bigtable:08,
	abstract = {Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this article, we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.},
	address = {New York, NY, USA},
	articleno = {4},
	author = {Chang, Fay and Dean, Jeffrey and Ghemawat, Sanjay and Hsieh, Wilson C. and Wallach, Deborah A. and Burrows, Mike and Chandra, Tushar and Fikes, Andrew and Gruber, Robert E.},
	date-added = {2023-03-04 15:46:53 +0300},
	date-modified = {2023-03-04 15:47:18 +0300},
	doi = {10.1145/1365815.1365816},
	issn = {0734-2071},
	issue_date = {June 2008},
	journal = {ACM Trans. Comput. Syst.},
	keywords = {Large-Scale Distributed Storage},
	month = {jun},
	number = {2},
	numpages = {26},
	publisher = {Association for Computing Machinery},
	title = {Bigtable: A Distributed Storage System for Structured Data},
	url = {https://doi.org/10.1145/1365815.1365816},
	volume = {26},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1145/1365815.1365816}}

@article{Mnih:AtariRL:13,
	author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin A. Riedmiller},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
	date-added = {2023-03-04 15:40:15 +0300},
	date-modified = {2023-03-04 15:40:34 +0300},
	eprint = {1312.5602},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
	title = {Playing Atari with Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1312.5602},
	volume = {abs/1312.5602},
	year = {2013},
	bdsk-url-1 = {http://arxiv.org/abs/1312.5602}}

@misc{pdp:23,
	author = {Public Domain Pictures},
	date-added = {2023-03-04 15:28:53 +0300},
	date-modified = {2023-03-04 15:29:53 +0300},
	howpublished = {\url{https://www.publicdomainpictures.net/en/view-image.php?image=308781&picture=earth-in-space}},
	note = {Accessed: 2023-03-02},
	title = {Earth in space}}

@misc{coc:23,
	author = {Qassim University},
	date-modified = {2023-03-04 01:12:47 +0300},
	howpublished = {\url{https://coc.qu.edu.sa/}},
	note = {Accessed: 2023-03-04},
	title = {College of Computer at Qassim University}}

@article{Turing:Computable:36,
	author = {Turing, Alan Mathison},
	date-modified = {2023-03-04 15:45:56 +0300},
	journal = {J. of Math},
	number = {345-363},
	pages = {5},
	title = {On computable numbers, with an application to the Entscheidungsproblem},
	url = {https://www.wolframscience.com/prizes/tm23/images/Turing.pdf},
	volume = {58},
	year = {1936},
	bdsk-url-1 = {https://www.wolframscience.com/prizes/tm23/images/Turing.pdf}}

@inproceedings{Sutskever:SeqToSeq:14,
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	address = {Cambridge, MA, USA},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
	date-modified = {2023-03-04 15:52:23 +0300},
	location = {Montreal, Canada},
	numpages = {9},
	pages = {3104--3112},
	publisher = {MIT Press},
	series = {NIPS'14},
	title = {Sequence to Sequence Learning with Neural Networks},
	url = {http://arxiv.org/abs/1409.3215},
	year = {2014},
	bdsk-url-1 = {http://arxiv.org/abs/1409.3215}}

@book{Knuth:Typesetting:86,
	author = {Donald Knuth},
	date-added = {2023-03-04 00:49:21 +0300},
	date-modified = {2023-03-04 00:53:48 +0300},
	isbn = {978-0201734164},
	publisher = {Addison-Wesley},
	title = {Computers and Typesetting},
	volume = {A},
	year = {1986}}
